{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f796de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAK\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data:  [[ -71.21212121   92.42424242    1.07992247]\n",
      " [ -65.15151515    4.54545455    1.76714587]\n",
      " [ -50.         -137.87878788    0.78539816]\n",
      " ...\n",
      " [ -83.33333333  -71.21212121    2.84706834]\n",
      " [-131.81818182  -31.81818182    4.90873852]\n",
      " [  31.81818182  134.84848485    2.94524311]]\n",
      "target data:  [[0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 0.]]\n",
      "input data shape:  (498420, 3)\n",
      "target data shape:  (498420, 3)\n",
      "Normalized data:  [[0.25773196 0.81443299 0.171875  ]\n",
      " [0.27835052 0.51546392 0.28125   ]\n",
      " [0.32989691 0.03092784 0.125     ]\n",
      " ...\n",
      " [0.21649485 0.25773196 0.453125  ]\n",
      " [0.05154639 0.39175258 0.78125   ]\n",
      " [0.60824742 0.95876289 0.46875   ]]\n",
      "Unique combinations:\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]]\n",
      "Encoded Labels:\n",
      "[2 0 2 4 2 0 4 3 0 1 2 0 5 3 3 0 5 4 5 5]\n",
      "One-Hot Encoded Labels:\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "Label to Combination Mapping:\n",
      "Label 0: (0.0, 0.0, 1.0)\n",
      "Label 1: (0.0, 1.0, 0.0)\n",
      "Label 2: (0.0, 1.0, 1.0)\n",
      "Label 3: (1.0, 0.0, 0.0)\n",
      "Label 4: (1.0, 0.0, 1.0)\n",
      "Label 5: (1.0, 1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('module://ipykernel.pylab.backend_inline') #used when working with the matplotlib library in Python within a Jupyter Notebook environment to save pgf plots effectively.\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Pre-processing data\n",
    "\n",
    "# Load data from .npy files\n",
    "input_data = np.load('group5_input_data.npy')\n",
    "target_data = np.load('group5_target_data.npy')\n",
    "print(\"input data: \", input_data)\n",
    "print(\"target data: \", target_data[:20])\n",
    "\n",
    "print(\"input data shape: \", input_data.shape)\n",
    "print(\"target data shape: \", target_data.shape)\n",
    "\n",
    "# Normalize data to improve convergence\n",
    "scaler = MinMaxScaler()\n",
    "normalized_input_data = scaler.fit_transform(input_data)\n",
    "print(\"Normalized data: \", normalized_input_data) \n",
    "\n",
    "# Observe unique combinations\n",
    "unique_combinations = np.unique(target_data, axis=0)\n",
    "print(\"Unique combinations:\")\n",
    "print(unique_combinations)\n",
    "\n",
    "# Assign integer labels\n",
    "label_map = {}\n",
    "for i, combo in enumerate(unique_combinations):\n",
    "    label_map[tuple(combo)] = i\n",
    "\n",
    "# Encode target data using the label map\n",
    "encoded_labels = np.array([label_map[tuple(combo)] for combo in target_data])\n",
    "\n",
    "print(\"Encoded Labels:\")\n",
    "print(encoded_labels[:20]) #Print first 20 to verify\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "one_hot_labels = to_categorical(encoded_labels)\n",
    "print(\"One-Hot Encoded Labels:\")\n",
    "print(one_hot_labels[:20]) #Print first 20 to verify\n",
    "\n",
    "# Print label-to-combination mapping just for further verification\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "print(\"Label to Combination Mapping:\")\n",
    "for label, combo in reverse_label_map.items():\n",
    "    print(f\"Label {label}: {combo}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c922f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/151\n",
      "7975/7975 - 17s - loss: 0.3731 - accuracy: 0.8724 - val_loss: 0.1750 - val_accuracy: 0.9379 - 17s/epoch - 2ms/step\n",
      "Epoch 2/151\n",
      "7975/7975 - 15s - loss: 0.1545 - accuracy: 0.9443 - val_loss: 0.1392 - val_accuracy: 0.9486 - 15s/epoch - 2ms/step\n",
      "Epoch 3/151\n",
      "7975/7975 - 16s - loss: 0.1327 - accuracy: 0.9513 - val_loss: 0.1179 - val_accuracy: 0.9577 - 16s/epoch - 2ms/step\n",
      "Epoch 4/151\n",
      "7975/7975 - 15s - loss: 0.1161 - accuracy: 0.9573 - val_loss: 0.1058 - val_accuracy: 0.9605 - 15s/epoch - 2ms/step\n",
      "Epoch 5/151\n",
      "7975/7975 - 16s - loss: 0.1079 - accuracy: 0.9598 - val_loss: 0.0983 - val_accuracy: 0.9638 - 16s/epoch - 2ms/step\n",
      "Epoch 6/151\n",
      "7975/7975 - 15s - loss: 0.1026 - accuracy: 0.9612 - val_loss: 0.1259 - val_accuracy: 0.9480 - 15s/epoch - 2ms/step\n",
      "Epoch 7/151\n",
      "7975/7975 - 14s - loss: 0.0989 - accuracy: 0.9624 - val_loss: 0.0996 - val_accuracy: 0.9602 - 14s/epoch - 2ms/step\n",
      "Epoch 8/151\n",
      "7975/7975 - 15s - loss: 0.0972 - accuracy: 0.9628 - val_loss: 0.0898 - val_accuracy: 0.9669 - 15s/epoch - 2ms/step\n",
      "Epoch 9/151\n",
      "7975/7975 - 16s - loss: 0.0946 - accuracy: 0.9638 - val_loss: 0.0886 - val_accuracy: 0.9657 - 16s/epoch - 2ms/step\n",
      "Epoch 10/151\n",
      "7975/7975 - 20s - loss: 0.0932 - accuracy: 0.9641 - val_loss: 0.0881 - val_accuracy: 0.9665 - 20s/epoch - 3ms/step\n",
      "Epoch 11/151\n",
      "7975/7975 - 19s - loss: 0.0919 - accuracy: 0.9640 - val_loss: 0.0949 - val_accuracy: 0.9623 - 19s/epoch - 2ms/step\n",
      "Epoch 12/151\n",
      "7975/7975 - 16s - loss: 0.0905 - accuracy: 0.9649 - val_loss: 0.0911 - val_accuracy: 0.9644 - 16s/epoch - 2ms/step\n",
      "Epoch 13/151\n",
      "7975/7975 - 18s - loss: 0.0896 - accuracy: 0.9650 - val_loss: 0.0836 - val_accuracy: 0.9669 - 18s/epoch - 2ms/step\n",
      "Epoch 14/151\n",
      "7975/7975 - 15s - loss: 0.0889 - accuracy: 0.9649 - val_loss: 0.0864 - val_accuracy: 0.9639 - 15s/epoch - 2ms/step\n",
      "Epoch 15/151\n",
      "7975/7975 - 17s - loss: 0.0875 - accuracy: 0.9650 - val_loss: 0.0819 - val_accuracy: 0.9668 - 17s/epoch - 2ms/step\n",
      "Epoch 16/151\n",
      "7975/7975 - 16s - loss: 0.0865 - accuracy: 0.9655 - val_loss: 0.0815 - val_accuracy: 0.9680 - 16s/epoch - 2ms/step\n",
      "Epoch 17/151\n",
      "7975/7975 - 14s - loss: 0.0854 - accuracy: 0.9654 - val_loss: 0.0796 - val_accuracy: 0.9682 - 14s/epoch - 2ms/step\n",
      "Epoch 18/151\n",
      "7975/7975 - 14s - loss: 0.0846 - accuracy: 0.9656 - val_loss: 0.0778 - val_accuracy: 0.9701 - 14s/epoch - 2ms/step\n",
      "Epoch 19/151\n",
      "7975/7975 - 15s - loss: 0.0836 - accuracy: 0.9660 - val_loss: 0.0959 - val_accuracy: 0.9598 - 15s/epoch - 2ms/step\n",
      "Epoch 20/151\n",
      "7975/7975 - 18s - loss: 0.0822 - accuracy: 0.9663 - val_loss: 0.0727 - val_accuracy: 0.9716 - 18s/epoch - 2ms/step\n",
      "Epoch 21/151\n",
      "7975/7975 - 14s - loss: 0.0819 - accuracy: 0.9663 - val_loss: 0.0794 - val_accuracy: 0.9676 - 14s/epoch - 2ms/step\n",
      "Epoch 22/151\n",
      "7975/7975 - 14s - loss: 0.0806 - accuracy: 0.9665 - val_loss: 0.0784 - val_accuracy: 0.9673 - 14s/epoch - 2ms/step\n",
      "Epoch 23/151\n",
      "7975/7975 - 17s - loss: 0.0804 - accuracy: 0.9668 - val_loss: 0.0788 - val_accuracy: 0.9673 - 17s/epoch - 2ms/step\n",
      "Epoch 24/151\n",
      "7975/7975 - 15s - loss: 0.0788 - accuracy: 0.9675 - val_loss: 0.0764 - val_accuracy: 0.9680 - 15s/epoch - 2ms/step\n",
      "Epoch 25/151\n",
      "7975/7975 - 18s - loss: 0.0785 - accuracy: 0.9671 - val_loss: 0.0849 - val_accuracy: 0.9641 - 18s/epoch - 2ms/step\n",
      "Epoch 26/151\n",
      "7975/7975 - 14s - loss: 0.0778 - accuracy: 0.9672 - val_loss: 0.0752 - val_accuracy: 0.9693 - 14s/epoch - 2ms/step\n",
      "Epoch 27/151\n",
      "7975/7975 - 14s - loss: 0.0770 - accuracy: 0.9680 - val_loss: 0.0853 - val_accuracy: 0.9624 - 14s/epoch - 2ms/step\n",
      "Epoch 28/151\n",
      "7975/7975 - 14s - loss: 0.0762 - accuracy: 0.9681 - val_loss: 0.0680 - val_accuracy: 0.9723 - 14s/epoch - 2ms/step\n",
      "Epoch 29/151\n",
      "7975/7975 - 14s - loss: 0.0756 - accuracy: 0.9683 - val_loss: 0.0823 - val_accuracy: 0.9637 - 14s/epoch - 2ms/step\n",
      "Epoch 30/151\n",
      "7975/7975 - 14s - loss: 0.0752 - accuracy: 0.9684 - val_loss: 0.0949 - val_accuracy: 0.9603 - 14s/epoch - 2ms/step\n",
      "Epoch 31/151\n",
      "7975/7975 - 14s - loss: 0.0748 - accuracy: 0.9686 - val_loss: 0.0787 - val_accuracy: 0.9678 - 14s/epoch - 2ms/step\n",
      "Epoch 32/151\n",
      "7975/7975 - 14s - loss: 0.0738 - accuracy: 0.9690 - val_loss: 0.0727 - val_accuracy: 0.9686 - 14s/epoch - 2ms/step\n",
      "Epoch 33/151\n",
      "7975/7975 - 14s - loss: 0.0731 - accuracy: 0.9692 - val_loss: 0.0656 - val_accuracy: 0.9730 - 14s/epoch - 2ms/step\n",
      "Epoch 34/151\n",
      "7975/7975 - 16s - loss: 0.0731 - accuracy: 0.9695 - val_loss: 0.0700 - val_accuracy: 0.9707 - 16s/epoch - 2ms/step\n",
      "Epoch 35/151\n",
      "7975/7975 - 17s - loss: 0.0724 - accuracy: 0.9699 - val_loss: 0.0695 - val_accuracy: 0.9700 - 17s/epoch - 2ms/step\n",
      "Epoch 36/151\n",
      "7975/7975 - 16s - loss: 0.0720 - accuracy: 0.9697 - val_loss: 0.0739 - val_accuracy: 0.9678 - 16s/epoch - 2ms/step\n",
      "Epoch 37/151\n",
      "7975/7975 - 16s - loss: 0.0717 - accuracy: 0.9702 - val_loss: 0.0631 - val_accuracy: 0.9740 - 16s/epoch - 2ms/step\n",
      "Epoch 38/151\n",
      "7975/7975 - 15s - loss: 0.0708 - accuracy: 0.9706 - val_loss: 0.0664 - val_accuracy: 0.9719 - 15s/epoch - 2ms/step\n",
      "Epoch 39/151\n",
      "7975/7975 - 17s - loss: 0.0706 - accuracy: 0.9704 - val_loss: 0.0782 - val_accuracy: 0.9664 - 17s/epoch - 2ms/step\n",
      "Epoch 40/151\n",
      "7975/7975 - 15s - loss: 0.0705 - accuracy: 0.9703 - val_loss: 0.0691 - val_accuracy: 0.9719 - 15s/epoch - 2ms/step\n",
      "Epoch 41/151\n",
      "7975/7975 - 14s - loss: 0.0709 - accuracy: 0.9702 - val_loss: 0.0640 - val_accuracy: 0.9725 - 14s/epoch - 2ms/step\n",
      "Epoch 42/151\n",
      "7975/7975 - 14s - loss: 0.0690 - accuracy: 0.9712 - val_loss: 0.0720 - val_accuracy: 0.9690 - 14s/epoch - 2ms/step\n",
      "Epoch 43/151\n",
      "7975/7975 - 17s - loss: 0.0693 - accuracy: 0.9711 - val_loss: 0.0740 - val_accuracy: 0.9686 - 17s/epoch - 2ms/step\n",
      "Epoch 44/151\n",
      "7975/7975 - 17s - loss: 0.0685 - accuracy: 0.9715 - val_loss: 0.0658 - val_accuracy: 0.9723 - 17s/epoch - 2ms/step\n",
      "Epoch 45/151\n",
      "7975/7975 - 17s - loss: 0.0679 - accuracy: 0.9718 - val_loss: 0.0672 - val_accuracy: 0.9728 - 17s/epoch - 2ms/step\n",
      "Epoch 46/151\n",
      "7975/7975 - 14s - loss: 0.0678 - accuracy: 0.9720 - val_loss: 0.0636 - val_accuracy: 0.9747 - 14s/epoch - 2ms/step\n",
      "Epoch 47/151\n",
      "7975/7975 - 14s - loss: 0.0674 - accuracy: 0.9722 - val_loss: 0.0757 - val_accuracy: 0.9669 - 14s/epoch - 2ms/step\n",
      "Epoch 48/151\n",
      "7975/7975 - 14s - loss: 0.0666 - accuracy: 0.9723 - val_loss: 0.0594 - val_accuracy: 0.9761 - 14s/epoch - 2ms/step\n",
      "Epoch 49/151\n",
      "7975/7975 - 14s - loss: 0.0656 - accuracy: 0.9727 - val_loss: 0.0660 - val_accuracy: 0.9711 - 14s/epoch - 2ms/step\n",
      "Epoch 50/151\n",
      "7975/7975 - 14s - loss: 0.0660 - accuracy: 0.9728 - val_loss: 0.0656 - val_accuracy: 0.9707 - 14s/epoch - 2ms/step\n",
      "Epoch 51/151\n",
      "7975/7975 - 14s - loss: 0.0646 - accuracy: 0.9735 - val_loss: 0.0586 - val_accuracy: 0.9758 - 14s/epoch - 2ms/step\n",
      "Epoch 52/151\n",
      "7975/7975 - 17s - loss: 0.0644 - accuracy: 0.9735 - val_loss: 0.0630 - val_accuracy: 0.9743 - 17s/epoch - 2ms/step\n",
      "Epoch 53/151\n",
      "7975/7975 - 14s - loss: 0.0640 - accuracy: 0.9737 - val_loss: 0.0771 - val_accuracy: 0.9685 - 14s/epoch - 2ms/step\n",
      "Epoch 54/151\n",
      "7975/7975 - 14s - loss: 0.0634 - accuracy: 0.9737 - val_loss: 0.0784 - val_accuracy: 0.9738 - 14s/epoch - 2ms/step\n",
      "Epoch 55/151\n",
      "7975/7975 - 14s - loss: 0.0633 - accuracy: 0.9739 - val_loss: 0.0572 - val_accuracy: 0.9760 - 14s/epoch - 2ms/step\n",
      "Epoch 56/151\n",
      "7975/7975 - 15s - loss: 0.0628 - accuracy: 0.9743 - val_loss: 0.0653 - val_accuracy: 0.9735 - 15s/epoch - 2ms/step\n",
      "Epoch 57/151\n",
      "7975/7975 - 15s - loss: 0.0626 - accuracy: 0.9743 - val_loss: 0.0643 - val_accuracy: 0.9738 - 15s/epoch - 2ms/step\n",
      "Epoch 58/151\n",
      "7975/7975 - 15s - loss: 0.0614 - accuracy: 0.9747 - val_loss: 0.0553 - val_accuracy: 0.9780 - 15s/epoch - 2ms/step\n",
      "Epoch 59/151\n",
      "7975/7975 - 15s - loss: 0.0618 - accuracy: 0.9744 - val_loss: 0.0549 - val_accuracy: 0.9779 - 15s/epoch - 2ms/step\n",
      "Epoch 60/151\n",
      "7975/7975 - 15s - loss: 0.0611 - accuracy: 0.9750 - val_loss: 0.0567 - val_accuracy: 0.9778 - 15s/epoch - 2ms/step\n",
      "Epoch 61/151\n",
      "7975/7975 - 19s - loss: 0.0602 - accuracy: 0.9754 - val_loss: 0.0541 - val_accuracy: 0.9767 - 19s/epoch - 2ms/step\n",
      "Epoch 62/151\n",
      "7975/7975 - 15s - loss: 0.0599 - accuracy: 0.9753 - val_loss: 0.0494 - val_accuracy: 0.9807 - 15s/epoch - 2ms/step\n",
      "Epoch 63/151\n",
      "7975/7975 - 14s - loss: 0.0588 - accuracy: 0.9757 - val_loss: 0.0508 - val_accuracy: 0.9792 - 14s/epoch - 2ms/step\n",
      "Epoch 64/151\n",
      "7975/7975 - 14s - loss: 0.0585 - accuracy: 0.9761 - val_loss: 0.0601 - val_accuracy: 0.9763 - 14s/epoch - 2ms/step\n",
      "Epoch 65/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7975/7975 - 14s - loss: 0.0579 - accuracy: 0.9762 - val_loss: 0.0537 - val_accuracy: 0.9778 - 14s/epoch - 2ms/step\n",
      "Epoch 66/151\n",
      "7975/7975 - 15s - loss: 0.0573 - accuracy: 0.9764 - val_loss: 0.0505 - val_accuracy: 0.9805 - 15s/epoch - 2ms/step\n",
      "Epoch 67/151\n",
      "7975/7975 - 15s - loss: 0.0561 - accuracy: 0.9770 - val_loss: 0.0512 - val_accuracy: 0.9803 - 15s/epoch - 2ms/step\n",
      "Epoch 68/151\n",
      "7975/7975 - 14s - loss: 0.0558 - accuracy: 0.9771 - val_loss: 0.0573 - val_accuracy: 0.9758 - 14s/epoch - 2ms/step\n",
      "Epoch 69/151\n",
      "7975/7975 - 14s - loss: 0.0552 - accuracy: 0.9774 - val_loss: 0.0477 - val_accuracy: 0.9802 - 14s/epoch - 2ms/step\n",
      "Epoch 70/151\n",
      "7975/7975 - 14s - loss: 0.0544 - accuracy: 0.9776 - val_loss: 0.0560 - val_accuracy: 0.9771 - 14s/epoch - 2ms/step\n",
      "Epoch 71/151\n",
      "7975/7975 - 14s - loss: 0.0539 - accuracy: 0.9779 - val_loss: 0.0836 - val_accuracy: 0.9632 - 14s/epoch - 2ms/step\n",
      "Epoch 72/151\n",
      "7975/7975 - 14s - loss: 0.0534 - accuracy: 0.9783 - val_loss: 0.0519 - val_accuracy: 0.9779 - 14s/epoch - 2ms/step\n",
      "Epoch 73/151\n",
      "7975/7975 - 14s - loss: 0.0526 - accuracy: 0.9786 - val_loss: 0.0657 - val_accuracy: 0.9724 - 14s/epoch - 2ms/step\n",
      "Epoch 74/151\n",
      "7975/7975 - 14s - loss: 0.0522 - accuracy: 0.9786 - val_loss: 0.0473 - val_accuracy: 0.9809 - 14s/epoch - 2ms/step\n",
      "Epoch 75/151\n",
      "7975/7975 - 14s - loss: 0.0514 - accuracy: 0.9789 - val_loss: 0.0455 - val_accuracy: 0.9818 - 14s/epoch - 2ms/step\n",
      "Epoch 76/151\n",
      "7975/7975 - 14s - loss: 0.0518 - accuracy: 0.9788 - val_loss: 0.0442 - val_accuracy: 0.9818 - 14s/epoch - 2ms/step\n",
      "Epoch 77/151\n",
      "7975/7975 - 14s - loss: 0.0504 - accuracy: 0.9793 - val_loss: 0.0567 - val_accuracy: 0.9746 - 14s/epoch - 2ms/step\n",
      "Epoch 78/151\n",
      "7975/7975 - 14s - loss: 0.0496 - accuracy: 0.9797 - val_loss: 0.0485 - val_accuracy: 0.9807 - 14s/epoch - 2ms/step\n",
      "Epoch 79/151\n",
      "7975/7975 - 14s - loss: 0.0496 - accuracy: 0.9797 - val_loss: 0.0662 - val_accuracy: 0.9724 - 14s/epoch - 2ms/step\n",
      "Epoch 80/151\n",
      "7975/7975 - 14s - loss: 0.0487 - accuracy: 0.9802 - val_loss: 0.0461 - val_accuracy: 0.9808 - 14s/epoch - 2ms/step\n",
      "Epoch 81/151\n",
      "7975/7975 - 14s - loss: 0.0484 - accuracy: 0.9802 - val_loss: 0.0408 - val_accuracy: 0.9842 - 14s/epoch - 2ms/step\n",
      "Epoch 82/151\n",
      "7975/7975 - 15s - loss: 0.0480 - accuracy: 0.9805 - val_loss: 0.0493 - val_accuracy: 0.9783 - 15s/epoch - 2ms/step\n",
      "Epoch 83/151\n",
      "7975/7975 - 14s - loss: 0.0484 - accuracy: 0.9802 - val_loss: 0.0441 - val_accuracy: 0.9830 - 14s/epoch - 2ms/step\n",
      "Epoch 84/151\n",
      "7975/7975 - 14s - loss: 0.0474 - accuracy: 0.9808 - val_loss: 0.0398 - val_accuracy: 0.9851 - 14s/epoch - 2ms/step\n",
      "Epoch 85/151\n",
      "7975/7975 - 14s - loss: 0.0476 - accuracy: 0.9808 - val_loss: 0.0396 - val_accuracy: 0.9851 - 14s/epoch - 2ms/step\n",
      "Epoch 86/151\n",
      "7975/7975 - 14s - loss: 0.0472 - accuracy: 0.9809 - val_loss: 0.0403 - val_accuracy: 0.9841 - 14s/epoch - 2ms/step\n",
      "Epoch 87/151\n",
      "7975/7975 - 14s - loss: 0.0466 - accuracy: 0.9812 - val_loss: 0.0469 - val_accuracy: 0.9816 - 14s/epoch - 2ms/step\n",
      "Epoch 88/151\n",
      "7975/7975 - 16s - loss: 0.0467 - accuracy: 0.9812 - val_loss: 0.0571 - val_accuracy: 0.9772 - 16s/epoch - 2ms/step\n",
      "Epoch 89/151\n",
      "7975/7975 - 16s - loss: 0.0468 - accuracy: 0.9810 - val_loss: 0.0550 - val_accuracy: 0.9798 - 16s/epoch - 2ms/step\n",
      "Epoch 90/151\n",
      "7975/7975 - 15s - loss: 0.0465 - accuracy: 0.9811 - val_loss: 0.0426 - val_accuracy: 0.9827 - 15s/epoch - 2ms/step\n",
      "Epoch 91/151\n",
      "7975/7975 - 14s - loss: 0.0455 - accuracy: 0.9816 - val_loss: 0.0443 - val_accuracy: 0.9824 - 14s/epoch - 2ms/step\n",
      "Epoch 92/151\n",
      "7975/7975 - 17s - loss: 0.0464 - accuracy: 0.9812 - val_loss: 0.0428 - val_accuracy: 0.9838 - 17s/epoch - 2ms/step\n",
      "Epoch 93/151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model using training data and in parallel validating using validation data\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m151\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For categorical cross entropy using one hot encoding \n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(normalized_input_data, one_hot_labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),  # Input layer\n",
    "    tf.keras.layers.Dense(12, activation='relu'),  # Hidden layer\n",
    "    tf.keras.layers.Dense(12, activation='relu'),   # Hidden layer\n",
    "    tf.keras.layers.Dense(12, activation='relu'),   # Hidden layer\n",
    "    tf.keras.layers.Dense(6, activation='softmax')  # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using training data and in parallel validating using validation data\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=151, batch_size=50, verbose=2, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('one_hot_loss.pgf', format='pgf') # Save plot for report\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('one_hot_accuracy.pgf', format='pgf')# Save plot for report\n",
    "\n",
    "# Confusion matrix - To validate training\n",
    "# model.predict gives the predicted probabilities for each class\n",
    "y_pred_prob = model.predict(X_train)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_true = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('one_hot_conf.pgf', format='pgf')# Save plot for report\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e51029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/151\n",
      "7975/7975 - 19s - loss: 0.4195 - accuracy: 0.8553 - val_loss: 0.1984 - val_accuracy: 0.9300 - 19s/epoch - 2ms/step\n",
      "Epoch 2/151\n",
      "7975/7975 - 15s - loss: 0.1811 - accuracy: 0.9342 - val_loss: 0.1668 - val_accuracy: 0.9439 - 15s/epoch - 2ms/step\n",
      "Epoch 3/151\n",
      "7975/7975 - 14s - loss: 0.1622 - accuracy: 0.9397 - val_loss: 0.1584 - val_accuracy: 0.9394 - 14s/epoch - 2ms/step\n",
      "Epoch 4/151\n",
      "7975/7975 - 15s - loss: 0.1534 - accuracy: 0.9417 - val_loss: 0.1486 - val_accuracy: 0.9434 - 15s/epoch - 2ms/step\n",
      "Epoch 5/151\n",
      "7975/7975 - 15s - loss: 0.1477 - accuracy: 0.9425 - val_loss: 0.1429 - val_accuracy: 0.9459 - 15s/epoch - 2ms/step\n",
      "Epoch 6/151\n",
      "7975/7975 - 14s - loss: 0.1437 - accuracy: 0.9432 - val_loss: 0.1373 - val_accuracy: 0.9462 - 14s/epoch - 2ms/step\n",
      "Epoch 7/151\n",
      "7975/7975 - 14s - loss: 0.1396 - accuracy: 0.9439 - val_loss: 0.1342 - val_accuracy: 0.9489 - 14s/epoch - 2ms/step\n",
      "Epoch 8/151\n",
      "7975/7975 - 14s - loss: 0.1363 - accuracy: 0.9444 - val_loss: 0.1385 - val_accuracy: 0.9449 - 14s/epoch - 2ms/step\n",
      "Epoch 9/151\n",
      "7975/7975 - 14s - loss: 0.1330 - accuracy: 0.9448 - val_loss: 0.1300 - val_accuracy: 0.9480 - 14s/epoch - 2ms/step\n",
      "Epoch 10/151\n",
      "7975/7975 - 14s - loss: 0.1296 - accuracy: 0.9457 - val_loss: 0.1343 - val_accuracy: 0.9441 - 14s/epoch - 2ms/step\n",
      "Epoch 11/151\n",
      "7975/7975 - 14s - loss: 0.1271 - accuracy: 0.9461 - val_loss: 0.1264 - val_accuracy: 0.9468 - 14s/epoch - 2ms/step\n",
      "Epoch 12/151\n",
      "7975/7975 - 14s - loss: 0.1249 - accuracy: 0.9467 - val_loss: 0.1255 - val_accuracy: 0.9451 - 14s/epoch - 2ms/step\n",
      "Epoch 13/151\n",
      "7975/7975 - 14s - loss: 0.1230 - accuracy: 0.9474 - val_loss: 0.1408 - val_accuracy: 0.9395 - 14s/epoch - 2ms/step\n",
      "Epoch 14/151\n",
      "7975/7975 - 14s - loss: 0.1214 - accuracy: 0.9482 - val_loss: 0.1182 - val_accuracy: 0.9495 - 14s/epoch - 2ms/step\n",
      "Epoch 15/151\n",
      "7975/7975 - 14s - loss: 0.1193 - accuracy: 0.9491 - val_loss: 0.1144 - val_accuracy: 0.9535 - 14s/epoch - 2ms/step\n",
      "Epoch 16/151\n",
      "7975/7975 - 14s - loss: 0.1185 - accuracy: 0.9490 - val_loss: 0.1283 - val_accuracy: 0.9468 - 14s/epoch - 2ms/step\n",
      "Epoch 17/151\n",
      "7975/7975 - 14s - loss: 0.1169 - accuracy: 0.9502 - val_loss: 0.1137 - val_accuracy: 0.9500 - 14s/epoch - 2ms/step\n",
      "Epoch 18/151\n",
      "7975/7975 - 14s - loss: 0.1159 - accuracy: 0.9500 - val_loss: 0.1145 - val_accuracy: 0.9514 - 14s/epoch - 2ms/step\n",
      "Epoch 19/151\n",
      "7975/7975 - 14s - loss: 0.1145 - accuracy: 0.9506 - val_loss: 0.1090 - val_accuracy: 0.9555 - 14s/epoch - 2ms/step\n",
      "Epoch 20/151\n",
      "7975/7975 - 14s - loss: 0.1136 - accuracy: 0.9511 - val_loss: 0.1200 - val_accuracy: 0.9520 - 14s/epoch - 2ms/step\n",
      "Epoch 21/151\n",
      "7975/7975 - 14s - loss: 0.1114 - accuracy: 0.9521 - val_loss: 0.1230 - val_accuracy: 0.9426 - 14s/epoch - 2ms/step\n",
      "Epoch 22/151\n",
      "7975/7975 - 14s - loss: 0.1103 - accuracy: 0.9525 - val_loss: 0.1124 - val_accuracy: 0.9531 - 14s/epoch - 2ms/step\n",
      "Epoch 23/151\n",
      "7975/7975 - 14s - loss: 0.1087 - accuracy: 0.9534 - val_loss: 0.1055 - val_accuracy: 0.9560 - 14s/epoch - 2ms/step\n",
      "Epoch 24/151\n"
     ]
    }
   ],
   "source": [
    "# For sparse categorical cross entropy - \n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(normalized_input_data, encoded_labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),  # Input layer\n",
    "    tf.keras.layers.Dense(12, activation='relu'),  # Hidden layer\n",
    "    tf.keras.layers.Dense(12, activation='relu'),   # Hidden layer\n",
    "    tf.keras.layers.Dense(12, activation='relu'),   # Hidden layer\n",
    "    tf.keras.layers.Dense(6, activation='softmax')  # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_sparse = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=151, batch_size=50, verbose=2, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history_sparse.history['loss'])\n",
    "plt.plot(history_sparse.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('sparse_loss.pgf', format='pgf')# Save plot for report\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history_sparse.history['accuracy'])\n",
    "plt.plot(history_sparse.history['val_accuracy'])\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('sparse_accuracy.pgf', format='pgf')# Save plot for report\n",
    "\n",
    "# Confusion matrix - To validate training\n",
    "# model.predict gives the predicted probabilities for each class\n",
    "y_pred_prob = model.predict(X_train)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_true = y_train\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('sparse_conf.pgf', format='pgf')# Save plot for report\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417bfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
